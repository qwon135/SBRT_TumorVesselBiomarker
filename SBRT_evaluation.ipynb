{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import v2\n",
    "import albumentations as A\n",
    "from datetime import datetime\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.preprocessing import MinMaxScaler, Binarizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from scipy.stats import pearsonr\n",
    "from mamba_ssm import Mamba\n",
    "import seaborn as sns\n",
    "import pickle, argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from models.feature_extractor import CT25D\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from TumorVesselDataset import CustomDataset, NormalVesselDataset\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score,confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from pydicom import dcmread\n",
    "import shutil\n",
    "import transformers\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from glob import glob\n",
    "from pycox.models.loss import cox_ph_loss, cox_cc_loss, nll_logistic_hazard, cox_ph_loss_sorted\n",
    "import random, timm\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return f'{mean:.3f}({mean - h:.2f}-{mean + h:.2f})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('SBRT_cohort.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform = A.Compose([\n",
    "    A.Resize(224, 224, interpolation=cv2.INTER_CUBIC),\n",
    "    ToTensorV2()\n",
    "        ], additional_targets={\n",
    "            'mask1': 'mask',\n",
    "            'mask2': 'mask'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    seed = 2\n",
    "    root = 'data'    \n",
    "    batch_size = 12\n",
    "    fold = 1\n",
    "    IMG_SIZE=224\n",
    "    num_workers = 4    \n",
    "    warmup_epoch = 10\n",
    "    device = 'cuda:0'\n",
    "    loss_type = 'BCE'    \n",
    "    cnn_lr = 5e-6\n",
    "    seq_lr = 5e-5\n",
    "    lr = 5e-5\n",
    "    weight_decay = 1e-3\n",
    "    ema_decay = 0.995\n",
    "    step=9\n",
    "    interval= 1    \n",
    "    ckpt = 'ckpt/best_RT.pt'    \n",
    "    \n",
    "args = CONFIG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbrt_dataset = CustomDataset(dataset, transform=valid_transform, mode='test')\n",
    "sbrt_loader = DataLoader(sbrt_dataset, shuffle = False, batch_size = 1, num_workers = args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CT25D().to(args.device)\n",
    "model.load_state_dict(torch.load(args.ckpt, 'cpu'), 'cpu')\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = os.listdir(f'data/normal_vessels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dataset = NormalVesselDataset(pids, valid_transform, 'test', args)\n",
    "normal_loader = DataLoader(normal_dataset, batch_size=32, shuffle=False, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_features = []\n",
    "for images in tqdm(normal_loader):\n",
    "    with torch.no_grad():\n",
    "        features = model.get_features(images.to(args.device)).cpu()\n",
    "    \n",
    "    normal_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MahalanobisDetector:\n",
    "    def __init__(self, feature_dim):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.mean = None\n",
    "        self.inv_covariance = None\n",
    "        \n",
    "    def fit(self, normal_features):\n",
    "        self.mean = torch.mean(normal_features, dim=0)\n",
    "        \n",
    "        centered_features = normal_features - self.mean\n",
    "        covariance = torch.mm(centered_features.t(), centered_features) / (normal_features.size(0) - 1)\n",
    "        \n",
    "        covariance_np = covariance.cpu().numpy()\n",
    "        epsilon = 1e-6\n",
    "        covariance_np += epsilon * np.eye(covariance_np.shape[0])\n",
    "        self.inv_covariance = torch.tensor(np.linalg.inv(covariance_np)).float()\n",
    "                \n",
    "        self.mean = self.mean\n",
    "        self.inv_covariance = self.inv_covariance\n",
    "\n",
    "    def calculate_distance(self, features):\n",
    "        centered_features = features - self.mean\n",
    "        \n",
    "        distances = torch.sqrt(\n",
    "            torch.sum(\n",
    "                torch.mm(centered_features, self.inv_covariance) * centered_features,\n",
    "                dim=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "detector = MahalanobisDetector(768)\n",
    "detector.fit(torch.cat(normal_features, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "mahalanobis_distance = []\n",
    "vessel_density = []\n",
    "\n",
    "for bs_img, bs_v_msks in tqdm(sbrt_loader):    \n",
    "    with torch.no_grad():\n",
    "        feature = model.get_features(bs_img.to(args.device)).cpu()\n",
    "    \n",
    "    mahalanobis_distance.append(detector.calculate_distance(feature).item())     \n",
    "    vessel_density.append(bs_v_msks.sum() / (224 * 224 * 3 * 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_risk_score = mahalanobis_distance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
